# SQLiteã‚’pysqlite3ã§ä¸Šæ›¸ã
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    print("Successfully overrode sqlite3 with pysqlite3")
except ImportError:
    print("Failed to override sqlite3 with pysqlite3")

import streamlit as st
import datetime

# æœ€åˆã®Streamlitã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦ãƒšãƒ¼ã‚¸è¨­å®šã‚’è¡Œã†
st.set_page_config(page_title='ğŸ¦œğŸ”— Ask the Doc App', layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'documents' not in st.session_state:
    st.session_state.documents = []
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None

from langchain_openai import OpenAI
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
# --- LLM --- (componentsãƒ•ã‚©ãƒ«ãƒ€ã«llm.pyã‚’é…ç½®ã™ã‚‹)---
from components.llm import llm
from components.llm import oai_embeddings
# --- LLM ---
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import tempfile
import os
import pandas as pd
# ChromaDBã¨VectorStoreã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯å¾Œã§è¡Œã† (SQLiteä¿®æ­£å¾Œ)
import io

# ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©
MAJOR_CATEGORIES = [
    "1. ç‰©ä»¶æ¦‚è¦",
    "2. åœ°åŸŸç‰¹æ€§ãƒ»è¡—ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«",
    "3. æ•™è‚²ãƒ»å­è‚²ã¦",
    "4. äº¤é€šãƒ»ã‚¢ã‚¯ã‚»ã‚¹",
    "5. å®‰å…¨ãƒ»é˜²ç½",
    "6. è¡Œæ”¿æ–½ç­–ãƒ»æ”¿ç­–",
    "7. ç”Ÿæ´»åˆ©ä¾¿æ€§",
    "8. ä¸å‹•ç”£å¸‚å ´",
    "9. åœ°åŸŸã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£",
    "10. ãã®ä»–ï¼ˆå€‹åˆ¥ã®æ‡¸å¿µãƒ»ç‰¹æ®Šäº‹æƒ…ï¼‰"
]

# ä¸­ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©
MEDIUM_CATEGORIES = {
    "1. ç‰©ä»¶æ¦‚è¦": [
        "1.1 å®Œæˆæ™‚æœŸ",
        "1.2 è²©å£²é–‹å§‹",
        "1.3 å»ºç¯‰ç¢ºèª",
        "1.4 é–“å–ã‚Šãƒ»ä»•æ§˜",
        "1.5 è¨­å‚™ãƒ»ã‚ªãƒ—ã‚·ãƒ§ãƒ³",
        "1.6 ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ»å¤–è¦³",
        "1.7 å»ºç¯‰ä¼šç¤¾ãƒ»ãƒ‡ãƒ™ãƒ­ãƒƒãƒ‘ãƒ¼",
        "1.8 ä¾¡æ ¼ãƒ»è²»ç”¨",
        "1.9 è³‡ç”£ä¾¡å€¤ãƒ»å£²å´",
        "1.10 å¥‘ç´„ãƒ»æ‰‹ç¶šã"
    ],
    "2. åœ°åŸŸç‰¹æ€§ãƒ»è¡—ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«": [
        "2.1 æ¦‚è¦ãƒ»ã‚¨ãƒªã‚¢åŒºåˆ†",
        "2.2 äººå£ãƒ»å±…ä½ç‰¹æ€§",
        "2.3 è¡—ã®æ­´å²ãƒ»åœ°åŸŸå²",
        "2.4 åœ°ç†çš„ç‰¹æ€§",
        "2.5 è‡ªç„¶ç’°å¢ƒ",
        "2.6 åœ°åŸŸã‚¤ãƒ™ãƒ³ãƒˆãƒ»ä¼çµ±è¡Œäº‹",
        "2.7 éƒ½å¸‚é€£æºãƒ»å§‰å¦¹éƒ½å¸‚æƒ…å ±",
        "2.8 æ²»å®‰ãƒ»é¨’éŸ³ãƒ»ç’°å¢ƒæ•´å‚™",
        "2.9 é¢¨æ™¯ãƒ»æ™¯è¦³ãƒ»è¡—ä¸¦ã¿",
        "2.10 è¦³å…‰ãƒ» åœ°å…ƒç‰¹ç”£å“ãƒ»åç”£ãƒ»ã‚°ãƒ«ãƒ¡"
    ],
    "3. æ•™è‚²ãƒ»å­è‚²ã¦": [
        "3.1 ä¿è‚²åœ’ãƒ»å¹¼ç¨šåœ’",
        "3.2 å°å­¦æ ¡ãƒ»ä¸­å­¦æ ¡",
        "3.3 å­¦ç«¥ãƒ»æ”¾èª²å¾Œæ”¯æ´",
        "3.4 ç¿’ã„äº‹ãƒ»å¡¾",
        "3.5 å­è‚²ã¦æ”¯æ´åˆ¶åº¦",
        "3.6 å…¬åœ’ãƒ»éŠã³å ´",
        "3.7 ç—…é™¢ãƒ»å°å…ç§‘",
        "3.8 å­¦åŒºãƒ»æ•™è‚²æ°´æº–",
        "3.9 å¾…æ©Ÿå…ç«¥ãƒ»å…¥åœ’çŠ¶æ³",
        "3.10 å­¦æ ¡ã‚¤ãƒ™ãƒ³ãƒˆãƒ»è¡Œäº‹"
    ],
    "4. äº¤é€šãƒ»ã‚¢ã‚¯ã‚»ã‚¹": [
        "4.1 æœ€å¯„ã‚Šé§…ãƒ»è·¯ç·š",
        "4.2 é›»è»Šã®æ··é›‘çŠ¶æ³",
        "4.3 ãƒã‚¹è·¯ç·šãƒ»æœ¬æ•°",
        "4.4 é§è¼ªå ´ãƒ»é§è»Šå ´",
        "4.5 é“è·¯äº¤é€šé‡ãƒ»æ¸‹æ»",
        "4.6 è»Šç§»å‹•ã®ã—ã‚„ã™ã•",
        "4.7 é€šå‹¤ãƒ»é€šå­¦æ™‚é–“",
        "4.8 é«˜é€Ÿé“è·¯ãƒ»ã‚¤ãƒ³ã‚¿ãƒ¼",
        "4.9 ã‚¿ã‚¯ã‚·ãƒ¼ãƒ»ãƒ©ã‚¤ãƒ‰ã‚·ã‚§ã‚¢",
        "4.10 ç©ºæ¸¯ãƒ»æ–°å¹¹ç·šã‚¢ã‚¯ã‚»ã‚¹"
    ],
    "5. å®‰å…¨ãƒ»é˜²ç½": [
        "5.1 é˜²çŠ¯ã‚«ãƒ¡ãƒ©ãƒ»äº¤ç•ªã®æœ‰ç„¡",
        "5.2 é¿é›£å ´æ‰€ãƒ»é˜²ç½æ‹ ç‚¹",
        "5.3 ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ï¼ˆæ´ªæ°´ãƒ»åœ°éœ‡ï¼‰",
        "5.4 åœŸç ‚ç½å®³ãƒªã‚¹ã‚¯",
        "5.5 è€éœ‡æ€§ãƒ»å»ºç‰©å¼·åº¦",
        "5.6 ç«ç½ãƒªã‚¹ã‚¯ãƒ»æ¶ˆé˜²ä½“åˆ¶",
        "5.7 å¤œé“ã®å®‰å…¨æ€§",
        "5.8 å°é¢¨ãƒ»é¢¨å®³ãƒ»é›ªå®³å¯¾ç­–",
        "5.9 åœ°éœ‡ãƒ»æ¶²çŠ¶åŒ–ãƒªã‚¹ã‚¯",
        "5.10 äº¤é€šäº‹æ•…ãƒ»å­ã©ã‚‚è¦‹å®ˆã‚Š"
    ],
    "6. è¡Œæ”¿æ–½ç­–ãƒ»æ”¿ç­–": [
        "6.1 å¸‚æ”¿ãƒ»è¡Œæ”¿çµ„ç¹”",
        "6.2 å†é–‹ç™ºãƒ»éƒ½å¸‚è¨ˆç”»",
        "6.3 äº¤é€šã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™",
        "6.4 å…¬å…±æ–½è¨­é‹å–¶ãƒ»å¸‚æ°‘ã‚µãƒ¼ãƒ“ã‚¹",
        "6.5 ã‚´ãƒŸåé›†ãƒ»æ¸…æƒç’°å¢ƒ",
        "6.6 å•†æ¥­ãƒ»ç”£æ¥­æŒ¯èˆˆç­–",
        "6.7 ä½å®…æ”¿ç­–ãƒ»ä½ç’°å¢ƒæ•´å‚™",
        "6.8 ç¦ç¥‰ãƒ»åŒ»ç™‚æ”¯æ´",
        "6.9 è£œåŠ©é‡‘ãƒ»åŠ©æˆåˆ¶åº¦",
        "6.10 è¡Œæ”¿è©•ä¾¡ãƒ»å¸‚æ°‘å‚åŠ "
    ],
    "7. ç”Ÿæ´»åˆ©ä¾¿æ€§": [
        "7.1 ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ»è²·ã„ç‰©ç’°å¢ƒ",
        "7.2 ã‚³ãƒ³ãƒ“ãƒ‹ãƒ»ãƒ‰ãƒ©ãƒƒã‚°ã‚¹ãƒˆã‚¢",
        "7.3 éŠ€è¡Œãƒ»é‡‘èæ©Ÿé–¢ãƒ»éƒµä¾¿å±€",
        "7.4 å…¬å…±æ–½è¨­",
        "7.5 ç—…é™¢ãƒ»ã‚¯ãƒªãƒ‹ãƒƒã‚¯ãƒ»å¤œé–“æ•‘æ€¥",
        "7.6 æ–‡åŒ–æ–½è¨­ãƒ»ç¾è¡“é¤¨ãƒ»åŠ‡å ´",
        "7.7 ã‚¹ãƒãƒ¼ãƒ„æ–½è¨­ãƒ»ã‚¸ãƒ ",
        "7.8 å¨¯æ¥½æ–½è¨­ãƒ»ã‚«ãƒ©ã‚ªã‚±ãƒ»æ˜ ç”»é¤¨",
        "7.9 é£²é£Ÿåº—ãƒ»ã‚°ãƒ«ãƒ¡ã‚¹ãƒãƒƒãƒˆ",
        "7.10 å®…é…ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒãƒƒãƒˆã‚¹ãƒ¼ãƒ‘ãƒ¼"
    ],
    "8. ä¸å‹•ç”£å¸‚å ´": [
        "8.1 åœ°ä¾¡ã®å¤‰å‹•ãƒ»æ¨ç§»",
        "8.2 å°†æ¥ã®å£²å´ã—ã‚„ã™ã•",
        "8.3 è³ƒè²¸éœ€è¦ãƒ»æŠ•è³‡ä¾¡å€¤",
        "8.4 ä½å®…ãƒ­ãƒ¼ãƒ³ãƒ»é‡‘åˆ©å‹•å‘",
        "8.5 äººæ°—ã‚¨ãƒªã‚¢ã®å‚¾å‘",
        "8.6 ç©ºãå®¶ãƒ»ä¸­å¤å¸‚å ´å‹•å‘",
        "8.7 å›ºå®šè³‡ç”£ç¨ãƒ»ç¨åˆ¶å„ªé‡",
        "8.8 ãƒãƒ³ã‚·ãƒ§ãƒ³ vs æˆ¸å»ºã¦",
        "8.9 ä½ã¿æ›¿ãˆãƒ»è»¢å‹¤æ™‚ã®å½±éŸ¿",
        "8.10 ä¸å‹•ç”£ä¼šç¤¾ã®è©•åˆ¤ãƒ»å®Ÿç¸¾"
    ],
    "9. åœ°åŸŸã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£": [
        "9.1 è‡ªæ²»ä¼šãƒ»ç”ºå†…ä¼šã®æ´»å‹•",
        "9.2 åœ°åŸŸã®ç¥­ã‚Š",
        "9.3 ãƒœãƒ©ãƒ³ãƒ†ã‚£ã‚¢æ´»å‹•",
        "9.4 ä½æ°‘ã®æ„è­˜ãƒ»å£ã‚³ãƒŸ",
        "9.5 é«˜é½¢è€…æ”¯æ´ãƒ»ç¦ç¥‰æ–½è¨­",
        "9.6 å¤–å›½äººã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£",
        "9.7 é¢¨è©•ãƒ»æ‚ªè©•ã®å®Ÿæ…‹",
        "9.8 å¸‚æ°‘å¹¸ç¦åº¦ãƒ»æº€è¶³åº¦",
        "9.9 åœ°åŸŸæŒ¯èˆˆãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»æ€§åŒ–"
    ],
    "10. ãã®ä»–ï¼ˆå€‹åˆ¥ã®æ‡¸å¿µãƒ»ç‰¹æ®Šäº‹æƒ…ï¼‰": [
        "10.1 ã‚¹ãƒãƒ¼ãƒˆã‚·ãƒ†ã‚£ãƒ»DXæ–½ç­–",
        "10.2 ã‚¨ã‚³å¯¾ç­–ãƒ»å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼",
        "10.3 è¦³å…‰å®¢ãƒ»çŸ­æœŸæ»åœ¨è€…ã®å½±éŸ¿",
        "10.4 æ™¯è¦³æ¡ä¾‹ãƒ»å»ºç¯‰è¦åˆ¶",
        "10.5 é§è»Šå ´å•é¡Œãƒ»è»Šä¸¡ãƒ«ãƒ¼ãƒ«",
        "10.6 å®…é…ä¾¿ãƒ»ç‰©æµã‚¤ãƒ³ãƒ•ãƒ©"
    ]
}

# ã‚«ã‚¹ã‚¿ãƒ RAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©
# langchainhubã«ä¾å­˜ã›ãšã«è‡ªå‰ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®šç¾©
RAG_PROMPT_TEMPLATE = """ã‚ãªãŸã¯ä¸å‹•ç”£ä¼šç¤¾ã®å–¶æ¥­æ‹…å½“ã§ã™ã€‚å–ã‚Šæ‰±ã£ã¦ã„ã‚‹ç‰©ä»¶ã‚’ä¸­å¿ƒã‚’ã—ãŸã‚¨ãƒªã‚¢ã«å¯¾ã—ã¦ã€ã‚¨ãƒªã‚¢ã®é­…åŠ›ã‚„ç‰¹å¾´ã€ç”Ÿæ´»ç’°å¢ƒã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã™ã‚‹ã“ã¨ãŒå¾—æ„ã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±æºã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å…·ä½“çš„ã§é­…åŠ›çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

æƒ…å ±æº:
{context}

è³ªå•: {question}

å›ç­”ã®éš›ã¯ä»¥ä¸‹ã®ãƒã‚¤ãƒ³ãƒˆã‚’æ„è­˜ã—ã¦ãã ã•ã„ï¼š
1. ã‚¨ãƒªã‚¢ã®é­…åŠ›ã‚„ç‰¹å¾´ã‚’å…·ä½“çš„ã«ä¼ãˆã‚‹
2. å®Ÿéš›ã«ä½ã‚€ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒæ¹§ãã‚ˆã†ãªèª¬æ˜ã‚’ã™ã‚‹
3. äº¤é€šã€æ•™è‚²ã€å•†æ¥­æ–½è¨­ã€åŒ»ç™‚ã€å…¬å…±æ–½è¨­ãªã©ã®ç”Ÿæ´»åˆ©ä¾¿æ€§ã«ã¤ã„ã¦è§¦ã‚Œã‚‹
4. æ•°å­—ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å®¢è¦³çš„ãªæƒ…å ±ã‚‚æä¾›ã™ã‚‹
5. ç‰©ä»¶ã®è¦‹å­¦æ„æ¬²ãŒé«˜ã¾ã‚‹ã‚ˆã†ãªè¡¨ç¾ã‚’ä½¿ã†
6. æƒ…å ±æºã«è¨˜è¼‰ãŒãªã„å†…å®¹ã«ã¤ã„ã¦ã¯ã€Œã“ã®ç‚¹ã«ã¤ã„ã¦ã¯æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ­£ç›´ã«ä¼ãˆã‚‹

å›ç­”:"""

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®åˆæœŸåŒ–
vector_store = None
vector_store_available = False

# VectorStoreã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°
def initialize_vector_store():
    global vector_store, vector_store_available
    if vector_store is not None:
        return vector_store
        
    try:
        from src.vector_store import VectorStore
        vector_store = VectorStore()
        vector_store_available = True
        print("VectorStore successfully initialized")
        return vector_store
    except Exception as e:
        vector_store_available = False
        print(f"Error initializing VectorStore: {e}")
        return None

# åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹
initialize_vector_store()

def register_document(uploaded_file, additional_metadata=None):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ChromaDBã«ç™»éŒ²ã™ã‚‹é–¢æ•°ã€‚
    additional_metadata: è¿½åŠ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¾æ›¸
    """
    if not vector_store_available:
        st.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ChromaDBãŒä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return
    
    if uploaded_file is not None:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿ - è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
            content = None
            encodings_to_try = ['utf-8', 'shift_jis', 'cp932', 'euc_jp', 'iso2022_jp']
            
            file_bytes = uploaded_file.getvalue()
            
            # ç•°ãªã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
            for encoding in encodings_to_try:
                try:
                    content = file_bytes.decode(encoding)
                    st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {encoding} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                    break
                except UnicodeDecodeError:
                    continue
            
            # ã©ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã‚‚èª­ã¿è¾¼ã‚ãªã‹ã£ãŸå ´åˆ
            if content is None:
                st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚UTF-8, Shift-JIS, EUC-JP, ISO-2022-JPã®ã„ãšã‚Œã‹ã§ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
                return
            
            # ãƒ¡ãƒ¢ãƒªå†…ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=512,
                chunk_overlap=10,
                add_start_index=True,
                separators=["\n\n", "\n", "ã€‚", ".", " ", ""],
            )
            
            # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            base_metadata = {'source': uploaded_file.name}
            
            # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯çµ±åˆ
            if additional_metadata:
                base_metadata.update(additional_metadata)
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
            from langchain_core.documents import Document
            raw_document = Document(
                page_content=content,
                metadata=base_metadata
            )
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²
            documents = text_splitter.split_documents([raw_document])

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜
            st.session_state.documents.extend(documents)

            # IDsã®ä½œæˆ
            original_ids = []
            for i, doc in enumerate(documents):
                source_ = os.path.splitext(uploaded_file.name)[0]  # æ‹¡å¼µå­ã‚’é™¤ã
                start_ = doc.metadata.get('start_index', i)
                id_str = f"{source_}_{start_:08}" #0ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦8æ¡ã«
                original_ids.append(id_str)

            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
            global vector_store
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¿½åŠ ï¼ˆUPSERTï¼‰
            vector_store.upsert_documents(documents=documents, ids=original_ids)

            st.success(f"{uploaded_file.name} ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ã—ã¾ã—ãŸã€‚")
            st.info(f"{len(documents)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            st.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç™»éŒ²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.error("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
            st.exception(e)

def manage_chromadb():
    """
    ChromaDBã‚’ç®¡ç†ã™ã‚‹ãƒšãƒ¼ã‚¸ã®é–¢æ•°ã€‚
    """
    st.header("ChromaDB ç®¡ç†")

    if not vector_store_available:
        st.error("ChromaDBã®æ¥ç¶šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç¾åœ¨ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.warning("ã“ã‚Œã¯SQLiteã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®éäº’æ›æ€§ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚Streamlit Cloudã§ã®å®Ÿè¡Œã«ã¯åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚")
        return

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®vector_storeã‚’ä½¿ç”¨
    global vector_store

    # 1.ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²
    st.subheader("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader('ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„', type='txt')
    
    if uploaded_file:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        with st.expander("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å…¥åŠ›", expanded=True):
            col1, col2 = st.columns(2)
            
            with col1:
                municipality = st.text_input("å¸‚åŒºç”ºæ‘å", "")
                major_category = st.selectbox(
                    "å¤§ã‚«ãƒ†ã‚´ãƒª",
                    MAJOR_CATEGORIES
                )
                medium_category = st.selectbox(
                    "ä¸­ã‚«ãƒ†ã‚´ãƒª",
                    MEDIUM_CATEGORIES.get(major_category, [])
                )
            
            with col2:
                source = st.text_input("ã‚½ãƒ¼ã‚¹å…ƒ", "")
                date_time = st.date_input("ç™»éŒ²æ—¥æ™‚", value=datetime.date.today())
                publication_date = st.date_input("ãƒ‡ãƒ¼ã‚¿å…¬é–‹æ—¥", value=None)
                latitude = st.text_input("ç·¯åº¦", "")
                longitude = st.text_input("çµŒåº¦", "")
        
        # ç™»éŒ²ãƒœã‚¿ãƒ³
        if st.button("ç™»éŒ²ã™ã‚‹"):
            with st.spinner('ç™»éŒ²ä¸­...'):
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                metadata = {
                    "municipality": municipality,
                    "major_category": major_category,
                    "medium_category": medium_category,
                    "source": source,
                    "registration_date": str(date_time) if date_time else "",
                    "publication_date": str(publication_date) if publication_date else "",
                    "latitude": latitude,
                    "longitude": longitude,
                }
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²é–¢æ•°ã‚’å‘¼ã³å‡ºã—
                register_document(uploaded_file, additional_metadata=metadata)

    st.markdown("---")

    # 2.ç™»éŒ²çŠ¶æ³ç¢ºèª
    st.subheader("ChromaDB ç™»éŒ²çŠ¶æ³ç¢ºèª")
    
    # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    with st.expander("æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", expanded=False):
        filter_municipality = st.text_input("å¸‚åŒºç”ºæ‘åã§çµã‚Šè¾¼ã¿", "")
        filter_category = st.text_input("ã‚«ãƒ†ã‚´ãƒªã§çµã‚Šè¾¼ã¿", "")
    
    # è¡¨ç¤ºãƒœã‚¿ãƒ³
    if st.button("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º"):
        with st.spinner('å–å¾—ä¸­...'):
            try:
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
                dict_data = vector_store.get_documents(ids=None)
                
                if dict_data and len(dict_data.get('ids', [])) > 0:
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    filtered_indices = range(len(dict_data['ids']))
                    
                    if filter_municipality or filter_category:
                        filtered_indices = []
                        for i, metadata in enumerate(dict_data['metadatas']):
                            municipality_match = True
                            category_match = True
                            
                            if filter_municipality and metadata.get('municipality'):
                                municipality_match = filter_municipality.lower() in metadata['municipality'].lower()
                            
                            if filter_category:
                                major_match = metadata.get('major_category') and filter_category.lower() in metadata['major_category'].lower()
                                medium_match = metadata.get('medium_category') and filter_category.lower() in metadata['medium_category'].lower()
                                category_match = major_match or medium_match
                                
                            if municipality_match and category_match:
                                filtered_indices.append(i)
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§DataFrameã‚’ä½œæˆ
                    filtered_ids = [dict_data['ids'][i] for i in filtered_indices]
                    filtered_docs = [dict_data['documents'][i] for i in filtered_indices]
                    filtered_metas = [dict_data['metadatas'][i] for i in filtered_indices]
                    
                    tmp_df = pd.DataFrame({
                        "IDs": filtered_ids,
                        "Documents": filtered_docs,
                        "å¸‚åŒºç”ºæ‘": [m.get('municipality', '') for m in filtered_metas],
                        "å¤§ã‚«ãƒ†ã‚´ãƒª": [m.get('major_category', '') for m in filtered_metas],
                        "ä¸­ã‚«ãƒ†ã‚´ãƒª": [m.get('medium_category', '') for m in filtered_metas],
                        "ã‚½ãƒ¼ã‚¹å…ƒ": [m.get('source', '') for m in filtered_metas],
                        "ç™»éŒ²æ—¥æ™‚": [m.get('registration_date', '') for m in filtered_metas],
                        "ãƒ‡ãƒ¼ã‚¿å…¬é–‹æ—¥": [m.get('publication_date', '') for m in filtered_metas],
                        "ç·¯åº¦çµŒåº¦": [f"{m.get('latitude', '')}, {m.get('longitude', '')}" for m in filtered_metas]
                    })
                    
                    st.dataframe(tmp_df)
                    st.success(f"åˆè¨ˆ {len(filtered_ids)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼ˆå…¨ {len(dict_data['ids'])} ä»¶ä¸­ï¼‰")
                else:
                    st.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.error("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
                st.exception(e)

    st.markdown("---")

    # 3.å…¨ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
    st.subheader("ChromaDB ç™»éŒ²ãƒ‡ãƒ¼ã‚¿å…¨å‰Šé™¤")
    if st.button("å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã™ã‚‹"):
        with st.spinner('å‰Šé™¤ä¸­...'):
            try:
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
                current_ids = vector_store.get_documents(ids=None).get('ids', [])
                if current_ids:
                    vector_store.delete_documents(ids=current_ids)
                    st.success(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ {len(current_ids)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
                else:
                    st.info("å‰Šé™¤ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.error("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
                st.exception(e)

# RAGã‚’ä½¿ã£ãŸLLMå›ç­”ç”Ÿæˆ
def generate_response(query_text, filter_conditions=None):
    """
    è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã€‚
    filter_conditions: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
    """
    if not vector_store_available:
        return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã§ããªã„ãŸã‚ã€è³ªå•ã«å›ç­”ã§ãã¾ã›ã‚“ã€‚"
    
    if query_text:
        try:
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
            global vector_store

            # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
            try:
                # ã¾ãšhub.pullã‚’è©¦ã™ï¼ˆlangchainhubãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                prompt = hub.pull("rlm/rag-prompt")
                print("Successfully pulled prompt from langchain hub")
            except (ImportError, Exception) as e:
                # å¤±æ•—ã—ãŸå ´åˆã¯è‡ªå‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
                print(f"Using custom prompt template due to: {e}")
                prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)

            # æ¤œç´¢çµæœã‚’å–å¾—ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ãŒã‚ã‚Œã°é©ç”¨ï¼‰
            search_results = vector_store.search(query_text, n_results=5, filter_conditions=filter_conditions)
            
            # æ¤œç´¢çµæœãŒãªã„å ´åˆ
            if not search_results or not search_results.get('documents', [[]])[0]:
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            # æ¤œç´¢çµæœã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå½¢å¼ã«å¤‰æ›
            from langchain_core.documents import Document
            docs = []
            for i, doc_text in enumerate(search_results['documents'][0]):
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
                metadata = {}
                if search_results.get('metadatas') and len(search_results['metadatas']) > 0:
                    metadata = search_results['metadatas'][0][i] if i < len(search_results['metadatas'][0]) else {}
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ
                doc = Document(
                    page_content=doc_text,
                    metadata=metadata
                )
                docs.append(doc)

            # ä½¿ç”¨ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’è¡¨ç¤º
            st.markdown("#### æ¤œç´¢çµæœ")
            meta_info = []
            for i, doc in enumerate(docs):
                meta_str = ""
                if doc.metadata.get('municipality'):
                    meta_str += f"ã€å¸‚åŒºç”ºæ‘ã€‘{doc.metadata['municipality']} "
                if doc.metadata.get('major_category'):
                    meta_str += f"ã€å¤§ã‚«ãƒ†ã‚´ãƒªã€‘{doc.metadata['major_category']} "
                if doc.metadata.get('medium_category'):
                    meta_str += f"ã€ä¸­ã‚«ãƒ†ã‚´ãƒªã€‘{doc.metadata['medium_category']} "
                if doc.metadata.get('source'):
                    meta_str += f"ã€ã‚½ãƒ¼ã‚¹å…ƒã€‘{doc.metadata['source']}"
                
                meta_info.append(f"{i+1}. {meta_str}")
            
            st.markdown("\n".join(meta_info))

            qa_chain = (
                {
                    "context": lambda x: format_docs(docs),
                    "question": RunnablePassthrough(),
                }
                | prompt
                | llm
                | StrOutputParser()
            )
            return qa_chain.invoke(query_text)
        except Exception as e:
            st.error(f"è³ªå•ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.error("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
            st.exception(e)
            return None

def ask_question():
    """
    è³ªå•ã™ã‚‹ãƒšãƒ¼ã‚¸ã®é–¢æ•°ã€‚
    """
    st.header("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«è³ªå•ã™ã‚‹")

    if not vector_store_available:
        st.error("ChromaDBã®æ¥ç¶šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç¾åœ¨ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
        st.warning("ã“ã‚Œã¯SQLiteã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®éäº’æ›æ€§ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚Streamlit Cloudã§ã®å®Ÿè¡Œã«ã¯åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚")
        st.info("ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®å®Ÿè¡Œã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
        return

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã®è¨­å®š
    with st.expander("æ¤œç´¢ç¯„å›²ã®çµã‚Šè¾¼ã¿", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            filter_municipality = st.text_input("å¸‚åŒºç”ºæ‘å", "")
            filter_major_category = st.selectbox(
                "å¤§ã‚«ãƒ†ã‚´ãƒª",
                [""] + MAJOR_CATEGORIES
            )
        
        with col2:
            filter_medium_category = st.selectbox(
                "ä¸­ã‚«ãƒ†ã‚´ãƒª",
                [""] + (MEDIUM_CATEGORIES.get(filter_major_category, []) if filter_major_category else [])
            )
            filter_source = st.text_input("ã‚½ãƒ¼ã‚¹å…ƒ", "")

    # Query text
    query_text = st.text_input('è³ªå•ã‚’å…¥åŠ›:', 
                               placeholder='ç°¡å˜ãªæ¦‚è¦ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„')

    # è³ªå•é€ä¿¡ãƒœã‚¿ãƒ³
    if st.button('Submit') and query_text:
        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã®ä½œæˆ
            filter_conditions = {}
            if filter_municipality:
                filter_conditions["municipality"] = filter_municipality
            if filter_major_category:
                filter_conditions["major_category"] = filter_major_category
            if filter_medium_category:
                filter_conditions["medium_category"] = filter_medium_category
            if filter_source:
                filter_conditions["source"] = filter_source
                
            response = generate_response(query_text, filter_conditions)
            if response:
                st.success("å›ç­”:")
                st.info(response)
            else:
                st.error("å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

def fallback_mode():
    """
    ChromaDBãŒä½¿ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰
    """
    st.header("ChromaDBãŒä½¿ç”¨ã§ãã¾ã›ã‚“")
    st.error("SQLiteã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å•é¡Œã«ã‚ˆã‚Šã€ChromaDBã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
    st.info("ã“ã®ã‚¢ãƒ—ãƒªã¯ã€SQLite 3.35.0ä»¥ä¸ŠãŒå¿…è¦ã§ã™ã€‚Streamlit Cloudã§ã¯ç¾åœ¨ã€SQLite 3.34.1ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    
    st.markdown("""
    ## è§£æ±ºç­–
    
    1. **ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®å®Ÿè¡Œ**: 
       - ã“ã®ã‚¢ãƒ—ãƒªã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„
       - æœ€æ–°ã®SQLiteãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„
    
    2. **ä»£æ›¿ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**:
       - ChromaDBã®ä»£ã‚ã‚Šã«ã€ä»–ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆFAISSã€Milvusãªã©ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚æ¤œè¨ã§ãã¾ã™
    
    3. **ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰ã§ã®ä½¿ç”¨**:
       - ç¾åœ¨ã€DuckDB+Parquetãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã®å®Ÿè¡Œã‚’è©¦ã¿ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚‚å¤±æ•—ã—ã¦ã„ã¾ã™
       - è©³ç´°ã«ã¤ã„ã¦ã¯ã€ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„
    """)
    
    # æŠ€è¡“çš„ãªè©³ç´°
    with st.expander("æŠ€è¡“çš„ãªè©³ç´°"):
        st.code("""
# ã‚¨ãƒ©ãƒ¼ã®åŸå› 
ChromaDBã¯å†…éƒ¨ã§SQLite 3.35.0ä»¥ä¸Šã‚’å¿…è¦ã¨ã—ã¦ã„ã¾ã™ãŒã€
Streamlit Cloudã§ã¯ç¾åœ¨ã€SQLite 3.34.1ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

# è©¦ã¿ãŸè§£æ±ºç­–
1. pysqlite3-binaryã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
2. SQLiteã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ“ãƒ«ãƒ‰
3. DuckDB+Parquetãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ä½¿ç”¨
4. ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒã®é©ç”¨

ã„ãšã‚Œã‚‚ç’°å¢ƒåˆ¶é™ã«ã‚ˆã‚ŠæˆåŠŸã—ã¦ã„ã¾ã›ã‚“ã€‚
        """)

def main():
    """
    ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚
    """
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
    st.title('ğŸ¦œğŸ”— Ask the Doc App')

    if not vector_store_available:
        fallback_mode()
        return

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒšãƒ¼ã‚¸é¸æŠ
    st.sidebar.title("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
    page = st.sidebar.radio("ãƒšãƒ¼ã‚¸ã‚’é¸æŠã—ã¦ãã ã•ã„", ["ChromaDB ç®¡ç†", "è³ªå•ã™ã‚‹",])

    # å„ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
    if page == "è³ªå•ã™ã‚‹":
        ask_question()
    elif page == "ChromaDB ç®¡ç†":
        manage_chromadb()

if __name__ == "__main__":
    main()
